{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will perform your first text mining analysis with the MeTA toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/41/34dd96bd33958e52cb4da2f1bf0818e396514fd4f4725a79199564cd0c20/pip-19.0.2-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 2.8MB/s ta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 18.0\n",
      "    Uninstalling pip-18.0:\n",
      "      Successfully uninstalled pip-18.0\n",
      "Successfully installed pip-19.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metapy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/cd/e5299611320b6ea281911cf871ccd91f04e2f71f2a434c7e6c5b5d7443bf/metapy-0.2.13-cp36-cp36m-macosx_10_6_intel.whl (13.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.5MB 2.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytoml\n",
      "  Downloading https://files.pythonhosted.org/packages/35/35/da1123673c54b6d701453fcd20f751d6a1fae43339b3993ae458875576e4/pytoml-0.1.20.tar.gz\n",
      "Building wheels for collected packages: pytoml\n",
      "  Building wheel for pytoml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/zhoukewei/Library/Caches/pip/wheels/d7/88/22/c6bd63ab856af808b7f7c2442fdd9eb8846027c35e37f9d9ee\n",
      "Successfully built pytoml\n",
      "Installing collected packages: metapy, pytoml\n",
      "Successfully installed metapy-0.2.13 pytoml-0.1.20\n"
     ]
    }
   ],
   "source": [
    "!pip install metapy pytoml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metapy\n",
    "metapy.log_to_stderr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = metapy.index.Document()\n",
    "doc.content(\"I said that I can't believe that it only costs $19.95!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "MeTA provides a stream-based interface for performing document tokenization.\n",
    "Each stream starts off with a Tokenizer object, and in most cases you should use the Unicode standard aware ICUTokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = metapy.analyzers.ICUTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers operate on raw text and provide an Iterable that spits out the individual text tokens.\n",
    "Let's try running just the ICUTokenizer to see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'I', 'said', 'that', 'I', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tok.set_content(doc.content()) # this could be any string\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One thing that you likely immediately notice is the insertion of these pseudo-XML looking tags.\n",
    "These are called “sentence boundary tags”.\n",
    "As a side-effect, a default-construted ICUTokenizer discovers the sentences in a document by delimiting them with the sentence boundary tags.\n",
    "Let's try tokenizing a multi-sentence document to see what that looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'I', 'said', 'that', 'I', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', '</s>', '<s>', 'I', 'could', 'only', 'find', 'it', 'for', 'more', 'than', '$', '30', 'before', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "doc.content(\"I said that I can't believe that it only costs $19.95! I could only find it for more than $30 before.\")\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the information retrieval techniques you have likely been learning about in this class don't need to concern themselves with finding the boundaries between separate sentences in a document, but later today we'll explore a scenario where this might matter more.\n",
    "Let's pass a flag to the ICUTokenizer constructor to disable sentence boundary tags for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'said', 'that', 'I', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', 'I', 'could', 'only', 'find', 'it', 'for', 'more', 'than', '$', '30', 'before', '.']\n"
     ]
    }
   ],
   "source": [
    "tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As mentioned earlier, MeTA treats tokenization as a streaming process, and that it starts with a tokenizer.\n",
    "It is often beneficial to modify the raw underlying tokens of a document, and thus change its representation.\n",
    "The “intermediate” steps in the tokenization stream are represented with objects called Filters.\n",
    "Each filter consumes the content of a previous filter (or a tokenizer) and modifies the tokens coming out of the stream in some way.\n",
    "Let's start by using a simple filter that can help eliminate a lot of noise that we might encounter when tokenizing web documents: a LengthFilter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['said', 'that', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '19.95', 'could', 'only', 'find', 'it', 'for', 'more', 'than', '30', 'before']\n"
     ]
    }
   ],
   "source": [
    "tok = metapy.analyzers.LengthFilter(tok, min=2, max=30)\n",
    "tok.set_content(doc.content())\n",
    "\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the LengthFilter is consuming our original ICUTokenizer.\n",
    "It modifies the token stream by only emitting tokens that are of a minimum length of 2 and a maximum length of 30.\n",
    "This can get rid of a lot of punctuation tokens, but also excessively long tokens such as URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lemur-stopwords.txt'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "wget.download('https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"can't\", 'believe', 'costs', '19.95', 'find', '30']\n"
     ]
    }
   ],
   "source": [
    "tok = metapy.analyzers.ListFilter(tok, \"lemur-stopwords.txt\", metapy.analyzers.ListFilter.Type.Reject)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've downloaded a common list of stopwords and created a ListFilter to reject any tokens that occur in that list of words.\n",
    "You can see how much of a difference removing stopwords can make on the size of a document's token stream!\n",
    "\n",
    "Another common filter that people use is called a stemmer, or lemmatizer.\n",
    "This kind of filter tries to modify individual tokens in such a way that different inflected forms of a word all reduce to the same representation.\n",
    "This lets you, for example, find documents about a “run” when you search “running” or “runs”.\n",
    "A common stemmer is the Porter2 Stemmer, which MeTA has an implementation of.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"can't\", 'believ', 'cost', '19.95', 'find', '30']\n"
     ]
    }
   ],
   "source": [
    "tok = metapy.analyzers.Porter2Filter(tok)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "Finally, after you've got the token stream configured the way you'd like, it's time to analyze the document by consuming each token from its token stream and performing some actions based on these tokens.\n",
    "In the simplest case, our action can simply be counting how many times these tokens occur.\n",
    "For clarity, let's switch back to a simpler token stream first.\n",
    "We will write a token stream that tokenizes with ICUTokenizer, and then lowercases each token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'said', 'that', 'i', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', 'i', 'could', 'only', 'find', 'it', 'for', 'more', 'than', '$', '30', 'before', '.']\n"
     ]
    }
   ],
   "source": [
    "tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)\n",
    "tok = metapy.analyzers.LowercaseFilter(tok)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's count how often each individual token appears in the stream.\n",
    "This representation is called “bag of words” representation or “unigram word counts”.\n",
    "In MeTA, classes that consume a token stream and emit a document representation are called Analyzers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I said that I can't believe that it only costs $19.95! I could only find it for more than $30 before.\n",
      "{'30': 1, 'more': 1, 'it': 2, 'before': 1, '.': 1, 'said': 1, 'only': 2, 'that': 2, 'find': 1, 'i': 3, '!': 1, '$': 2, 'for': 1, 'could': 1, \"can't\": 1, '19.95': 1, 'costs': 1, 'than': 1, 'believe': 1}\n"
     ]
    }
   ],
   "source": [
    "ana = metapy.analyzers.NGramWordAnalyzer(1, tok)\n",
    "print(doc.content())\n",
    "unigrams = ana.analyze(doc)\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you noticed the name of the analyzer, you might have realized that you can count not just individual tokens, but groups of them.\n",
    "“Unigram” means “1-gram”, and we count individual tokens. “Bigram” means “2-gram”, and we count adjacent tokens together as a group.\n",
    "Let's try that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('$', '30'): 1, ('it', 'only'): 1, ('than', '$'): 1, (\"can't\", 'believe'): 1, ('before', '.'): 1, ('i', 'could'): 1, ('that', 'it'): 1, ('it', 'for'): 1, ('believe', 'that'): 1, ('i', \"can't\"): 1, ('said', 'that'): 1, ('19.95', '!'): 1, ('could', 'only'): 1, ('i', 'said'): 1, ('more', 'than'): 1, ('$', '19.95'): 1, ('for', 'more'): 1, ('costs', '$'): 1, ('30', 'before'): 1, ('!', 'i'): 1, ('find', 'it'): 1, ('only', 'find'): 1, ('that', 'i'): 1, ('only', 'costs'): 1}\n"
     ]
    }
   ],
   "source": [
    "ana = metapy.analyzers.NGramWordAnalyzer(2, tok)\n",
    "bigrams = ana.analyze(doc)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the individual “tokens” we're counting are pairs of tokens.\n",
    "Sometimes looking at n-grams of characters is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('i', 't', ' ', 'f'): 1, ('1', '9', '.', '9'): 1, ('l', 'y', ' ', 'c'): 1, (' ', 'b', 'e', 'f'): 1, ('i', 'n', 'd', ' '): 1, ('o', 'u', 'l', 'd'): 1, ('t', ' ', 'i', 't'): 1, ('f', 'o', 'r', 'e'): 1, ('y', ' ', 'c', 'o'): 1, ('$', '3', '0', ' '): 1, ('s', 'a', 'i', 'd'): 1, ('y', ' ', 'f', 'i'): 1, (' ', 'b', 'e', 'l'): 1, ('5', '!', ' ', 'I'): 1, ('e', ' ', 't', 'h'): 2, ('l', 'i', 'e', 'v'): 1, ('u', 'l', 'd', ' '): 1, ('n', 'l', 'y', ' '): 2, ('a', 'i', 'd', ' '): 1, (' ', 'I', ' ', 'c'): 2, ('a', 'n', \"'\", 't'): 1, ('n', ' ', '$', '3'): 1, (\"'\", 't', ' ', 'b'): 1, ('b', 'e', 'f', 'o'): 1, ('o', 'r', ' ', 'm'): 1, ('o', 'r', 'e', ' '): 1, ('n', 'd', ' ', 'i'): 1, ('3', '0', ' ', 'b'): 1, ('t', 'h', 'a', 't'): 2, ('a', 't', ' ', 'i'): 1, ('b', 'e', 'l', 'i'): 1, ('i', 't', ' ', 'o'): 1, ('a', 't', ' ', 'I'): 1, ('o', 's', 't', 's'): 1, (' ', 'f', 'i', 'n'): 1, (' ', 'f', 'o', 'r'): 1, ('t', 's', ' ', '$'): 1, ('e', 'v', 'e', ' '): 1, (' ', 'c', 'a', 'n'): 1, ('t', ' ', 'b', 'e'): 1, ('n', \"'\", 't', ' '): 1, ('c', 'o', 's', 't'): 1, ('.', '9', '5', '!'): 1, ('v', 'e', ' ', 't'): 1, ('e', 'f', 'o', 'r'): 1, ('l', 'y', ' ', 'f'): 1, ('r', ' ', 'm', 'o'): 1, ('I', ' ', 'c', 'o'): 1, ('9', '.', '9', '5'): 1, (' ', '$', '1', '9'): 1, ('0', ' ', 'b', 'e'): 1, (' ', 's', 'a', 'i'): 1, ('t', 'h', 'a', 'n'): 1, ('I', ' ', 'c', 'a'): 1, ('!', ' ', 'I', ' '): 1, ('$', '1', '9', '.'): 1, ('f', 'i', 'n', 'd'): 1, (' ', 'c', 'o', 's'): 1, ('i', 'e', 'v', 'e'): 1, ('d', ' ', 'o', 'n'): 1, ('c', 'a', 'n', \"'\"): 1, (' ', 'i', 't', ' '): 2, ('t', ' ', 'f', 'o'): 1, ('i', 'd', ' ', 't'): 1, (' ', 't', 'h', 'a'): 3, ('o', 'n', 'l', 'y'): 2, ('t', ' ', 'I', ' '): 1, (' ', 'c', 'o', 'u'): 1, ('s', ' ', '$', '1'): 1, ('h', 'a', 'n', ' '): 1, ('r', 'e', ' ', 't'): 1, ('l', 'd', ' ', 'o'): 1, ('d', ' ', 't', 'h'): 1, ('f', 'o', 'r', ' '): 1, ('I', ' ', 's', 'a'): 1, (' ', 'o', 'n', 'l'): 2, ('t', ' ', 'o', 'n'): 1, ('d', ' ', 'i', 't'): 1, ('h', 'a', 't', ' '): 2, ('c', 'o', 'u', 'l'): 1, (' ', 'm', 'o', 'r'): 1, ('m', 'o', 'r', 'e'): 1, ('9', '5', '!', ' '): 1, (' ', '$', '3', '0'): 1, ('o', 'r', 'e', '.'): 1, ('s', 't', 's', ' '): 1, ('a', 'n', ' ', '$'): 1, ('e', 'l', 'i', 'e'): 1}\n"
     ]
    }
   ],
   "source": [
    "tok = metapy.analyzers.CharacterTokenizer()\n",
    "ana = metapy.analyzers.NGramWordAnalyzer(4, tok)\n",
    "fourchar_ngrams = ana.analyze(doc)\n",
    "print(fourchar_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging\n",
    "\n",
    "Now, let's explore something a little bit different.\n",
    "MeTA also has a natural language processing (NLP) component, which currently supports two major NLP tasks: part-of-speech tagging and syntactic parsing.\n",
    "POS tagging is a task in NLP that involves identifying a type for each word in a sentence.\n",
    "For example, POS tagging can be used to identify all of the nouns in a sentence, or all of the verbs, or adjectives, or…\n",
    "This is useful as first step towards developing an understanding of the meaning of a particular sentence.\n",
    "MeTA places its POS tagging component in its “sequences” library.\n",
    "Let's play with some sequences first to get an idea of how they work.\n",
    "We'll start of by creating a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = metapy.sequence.Sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can add individual words to this sequence.\n",
    "Sequences consist of a list of Observations, which are essentially (word, tag) pairs.\n",
    "If we don't yet know the tags for a Sequence, we can just add individual words and leave the tags unset.\n",
    "Words are called “symbols” in the library terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(The, ???), (dog, ???), (ran, ???), (across, ???), (the, ???), (park, ???), (., ???)\n"
     ]
    }
   ],
   "source": [
    "for word in [\"The\", \"dog\", \"ran\", \"across\", \"the\", \"park\", \".\"]:\n",
    "    seq.add_symbol(word)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printed form of the sequence shows that we do not yet know the tags for each word.\n",
    "Let's fill them in by using a pre-trained POS-tagger model that's distributed with MeTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'greedy-perceptron-tagger.tar.gz'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download('https://github.com/meta-toolkit/meta/releases/download/v3.0.1/greedy-perceptron-tagger.tar.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf = tarfile.open(\"greedy-perceptron-tagger.tar.gz\")\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r",
      " > Loading feature mapping: [>                               ]   0% ETA 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(The, DT), (dog, NN), (ran, VBD), (across, IN), (the, DT), (park, NN), (., .)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r",
      " > Loading feature mapping: [================================] 100% ETA 00:00:00 \n"
     ]
    }
   ],
   "source": [
    "tagger = metapy.sequence.PerceptronTagger(\"perceptron-tagger/\")\n",
    "tagger.tag(seq)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each tag indicates the type of a word, and this particular tagger was trained to output the tags present in the Penn Treebank tagset.\n",
    "But what if we want to POS-tag a document?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'I', 'said', 'that', 'I', 'ca', \"n't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', '</s>']\n"
     ]
    }
   ],
   "source": [
    "doc = metapy.index.Document()\n",
    "doc.content(\"I said that I can't believe that it only costs $19.95!\")\n",
    "tok = metapy.analyzers.ICUTokenizer() # keep sentence boundaries!\n",
    "tok = metapy.analyzers.PennTreebankNormalizer(tok)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will write a function that can take a token stream that contains sentence boundary tags and returns a list of Sequence objects.\n",
    "We will not include the sentence boundary tags in the actual Sequence objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(I, PRP), (said, VBD), (that, IN), (I, PRP), (ca, MD), (n't, RB), (believe, VB), (that, IN), (it, PRP), (only, RB), (costs, VBZ), ($, $), (19.95, CD), (!, .)\n"
     ]
    }
   ],
   "source": [
    "def extract_sequences(tok):\n",
    "    sequences = []\n",
    "    for token in tok:\n",
    "        if token == '<s>':\n",
    "            sequences.append(metapy.sequence.Sequence())\n",
    "        elif token != '</s>':\n",
    "            sequences[-1].add_symbol(token)\n",
    "    return sequences\n",
    "\n",
    "doc = metapy.index.Document()\n",
    "doc.content(\"I said that I can't believe that it only costs $19.95!\")\n",
    "tok.set_content(doc.content())\n",
    "for seq in extract_sequences(tok):\n",
    "    tagger.tag(seq)\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config.toml file: setting up a pipeline\n",
    "\n",
    "In practice, it is often beneficial to combine multiple feature sets together.\n",
    "We can do this with a MultiAnalyzer. Let's combine unigram words, bigram POS tags, and rewrite rules for our document feature representation.\n",
    "We can certainly do this programmatically, but doing so can become tedious quite quickly.\n",
    "Instead, let's use MeTA's configuration file format to specify our analyzer, which we can then load in one line of code.\n",
    "MeTA uses TOML configuration files for all of its configuration. If you haven't heard of TOML before, don't panic! It's a very simple, readable format.\n",
    "Open a text editor and copy the text below, but be careful not to modify the contents. Save it as `config.toml` .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crf.tar.gz'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download('https://github.com/meta-toolkit/meta/releases/download/v3.0.2/crf.tar.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tarfile.open(\"crf.tar.gz\")\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download('https://github.com/meta-toolkit/meta/releases/download/v3.0.2/greedy-constituency-parser.tar.gz')\n",
    "tf = tarfile.open(\"greedy-constituency-parser.tar.gz\")\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Loading feature mapping: [================================] 100% ETA 00:00:00 \n",
      " > Loading feature mapping: [================================] 100% ETA 00:00:00 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subtree-(VP (VBZ) (NP))': 1, 'subtree-(ADVP (RB))': 1, 'subtree-(VB)': 1, 'VBD_IN': 1, 'subtree-(NP (PRP))': 3, 'subtree-(VBD)': 1, 'PRP_VBD': 1, 'subtree-(VP (VB) (SBAR))': 1, 'subtree-(CD)': 1, 'subtree-(.)': 1, 'subtree-(S (NP) (ADVP) (VP))': 1, '$_CD': 1, 'PRP_MD': 1, 'cost': 1, 'subtree-(IN)': 2, 'subtree-($)': 1, 'subtree-(S (NP) (VP))': 1, 'believ': 1, 'subtree-(VP (MD) (RB) (VP))': 1, 'IN_PRP': 2, 'subtree-(S (NP) (VP) (.))': 1, 'subtree-(ROOT (S))': 1, 'subtree-(RB)': 2, 'subtree-(SBAR (IN) (S))': 2, \"can't\": 1, 'subtree-(PRP)': 3, 'VBZ_$': 1, 'subtree-(NP ($) (CD))': 1, 'VB_IN': 1, 'PRP_RB': 1, 'subtree-(VBZ)': 1, 'CD_.': 1, 'subtree-(MD)': 1, 'subtree-(VP (VBD) (SBAR))': 1, 'RB_VBZ': 1, 'MD_RB': 1, 'RB_VB': 1}\n"
     ]
    }
   ],
   "source": [
    "ana = metapy.analyzers.load('config.toml')\n",
    "doc = metapy.index.Document()\n",
    "doc.content(\"I said that I can't believe that it only costs $19.95!\")\n",
    "print(ana.analyze(doc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
